{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NOEk3_YPdg6p"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZK7M8-wZ_J8"
      },
      "source": [
        "# Project Notebook: Augmenting Pandas with SQLite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qadTA4HuceEl"
      },
      "source": [
        "## Question 1: Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF9k8rixclSR"
      },
      "source": [
        "In this session, we explored a few different ways to work with larger datasets in pandas. In this guided project, we'll practice using some of the techniques we learned to analyze startup investments from Crunchbase.com.\n",
        "\n",
        "Every year, thousands of startup companies raise financing from investors. Each time a startup raises money, we refer to the event as a fundraising round. Crunchbase is a website that crowdsources information on the fundraising rounds of many startups. The Crunchbase user community submits, edits, and maintains most of the information in Crunchbase.\n",
        "\n",
        "In return, Crunchbase makes the data available through a Web application and a fee-based API. Before Crunchbase switched to the paid API model, multiple groups crawled the site and released the data online. Because the information on the startups and their fundraising rounds is always changing, the data set we'll be using isn't completely up to date.\n",
        "\n",
        "Throughout this project, we'll practice working with different memory constraints. In this step, let's assume we only have 10 megabytes of available memory. While crunchbase-investments.csv (https://bit.ly/3BPcobU) consumes 10.3 megabytes of disk space, we know from earlier lessons that pandas often requires 4 to 6 times amount of space in memory as the file does on disk (especially when there's many string columns).\n",
        "\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "* Because the data set contains over 50,000 rows, you'll need to read the data set into dataframes using 5,000 row chunks to ensure that each chunk consumes much less than 10 megabytes of memory.\n",
        "* Across all of the chunks, become familiar with:\n",
        "1. Each column's missing value counts.\n",
        "2. Each column's memory footprint.\n",
        "3. The total memory footprint of all of the chunks combined.\n",
        "4. Which column(s) we can drop because they aren't useful for analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOYoNl5qe1ow"
      },
      "source": [
        "# Your code goes here\n",
        "import pandas as pd\n",
        "\n",
        "# Dataset URL = https://bit.ly/3BPcobU\n",
        "#\n",
        "\n",
        "# Open the dataset in chunks with a chunk size of 5000 rows\n",
        "\n",
        "df_chunk =  pd.read_csv(\"https://bit.ly/3BPcobU\", chunksize=5000)\n",
        "\n",
        "\n",
        "# Process each chunk\n",
        "for chunk in chunks:\n",
        "    # Get some information about the chunk\n",
        "    print(\"Shape:\", chunk.shape)  # Prints the number of rows and columns in the chunk\n",
        "    print(\"Columns:\", chunk.columns)  # Prints the column names\n",
        "    print(\"\\n\")  # Adds a blank line between chunks\n",
        "    # Perform any other operations you need to on the chunk\n",
        "    # ...\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the data into a dataframe\n",
        "df = pd.read_csv(\"https://bit.ly/3H2XVgC\")\n",
        "\n",
        "# Find the rows with missing values for each column\n",
        "null_counts = df.isnull().sum()\n",
        "\n",
        "# Print the results\n",
        "print(null_counts)\n",
        "\n"
      ],
      "metadata": {
        "id": "nrzcoUyfjZwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the data into a dataframe\n",
        "df = pd.read_csv(\"https://bit.ly/3H2XVgC\")\n",
        "\n",
        "# Get the memory usage of each column\n",
        "memory_footprint = df.memory_usage()\n",
        "\n",
        "# Print the results\n",
        "print(memory_footprint)\n",
        "\n"
      ],
      "metadata": {
        "id": "X6URX2a9jw0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Open the dataset in chunks with a chunk size of 5000 rows\n",
        "chunks = pd.read_csv(\"https://bit.ly/3H2XVgC\", chunksize=5000)\n",
        "\n",
        "# Initialize a variable to hold the total memory usage\n",
        "total_memory = 0\n",
        "\n",
        "# Process each chunk\n",
        "for chunk in chunks:\n",
        "    # Get the memory usage of the chunk\n",
        "    chunk_memory = chunk.memory_usage().sum()\n",
        "    # Add the chunk's memory usage to the total\n",
        "    total_memory += chunk_memory\n",
        "\n",
        "# Print the total memory usage\n",
        "print(\"Total memory usage:\", total_memory, \"bytes\")\n",
        "\n"
      ],
      "metadata": {
        "id": "yGYV6vMNkLzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the data into a dataframe\n",
        "df = pd.read_csv(\"https://bit.ly/3H2XVgC\")\n",
        "\n",
        "# Calculate the percentage of missing values in each column\n",
        "missing_values_percent = df.isnull().mean()\n",
        "\n",
        "# Drop columns that have more than 50% missing values\n",
        "df = df.drop(columns=missing_values_percent[missing_values_percent > 0.5].index)\n",
        "\n"
      ],
      "metadata": {
        "id": "74TwNza8kqsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOEk3_YPdg6p"
      },
      "source": [
        "## Question 2: Selecting Data Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD1IqmzhcEd3"
      },
      "source": [
        "Now that we have a good sense of the missing values, let's get familiar with the column types before adding the data into SQLite.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "* Identify the types for each column.\n",
        "* Identify the numeric columns we can represent using more space efficient types.\n",
        "For text columns:\n",
        "* Analyze the unique value counts across all of the chunks to see if we can convert them to a numeric type.\n",
        "* See if we clean clean any text columns and separate them into multiple numeric columns without adding any overhead when querying.\n",
        "* Make your changes to the code from the last step so that the overall memory the data consumes stays under 10 megabytes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNgzuiStZ3WN"
      },
      "source": [
        "# Your code goes here\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://bit.ly/3H2XVgC\")\n",
        "print(df.dtypes)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://bit.ly/3H2XVgC\")\n",
        "text_columns = df.select_dtypes(include=['object'])\n",
        "\n",
        "for column in text_columns:\n",
        "    print(f'Column {column}: {text_columns[column].dtype}')\n",
        "\n"
      ],
      "metadata": {
        "id": "j3wSGKQnofdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://bit.ly/3H2XVgC\")\n",
        "unique_counts = df.nunique()\n",
        "print(unique_counts)\n",
        "\n"
      ],
      "metadata": {
        "id": "aLjHUJEgpjef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://bit.ly/3H2XVgC\")\n",
        "df_dummies = pd.get_dummies(df, columns=['text_column'])\n",
        "\n"
      ],
      "metadata": {
        "id": "8xKzVBctpxXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://bit.ly/3H2XVgC\")\n",
        "memory_usage = df.memory_usage(deep=True)\n",
        "\n",
        "# Select columns that are consuming a large amount of memory\n",
        "high_memory_columns = memory_usage[memory_usage > 10_000_000].index\n",
        "\n",
        "# Convert high memory columns to categorical data type\n",
        "df[high_memory_columns] = df[high_memory_columns].astype('category')\n",
        "\n",
        "df_dummies = pd.get_dummies(df, columns=['text_column'])\n",
        "df_cleaned = pd.concat([df, df_dummies], axis=1)"
      ],
      "metadata": {
        "id": "dVQfIxzXryVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA2D7o4sds1t"
      },
      "source": [
        "## Question 3: Loading Chunks Into SQLite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptu8Xr25dvcF"
      },
      "source": [
        "Now we're in good shape to start exploring and analyzing the data. The next step is to load each chunk into a table in a SQLite database so we can query the full data set.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Create and connect to a new SQLite database file.\n",
        "2. Expand on the existing chunk processing code to export each chunk to a new table in the SQLite database.\n",
        "3. Query the table and make sure the data types match up to what you had in mind for each column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH-j6fuXdu0v"
      },
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Connect to the database\n",
        "conn = sqlite3.connect('database.db')\n",
        "\n",
        "# Load the data into the table\n",
        "df.to_sql('table_name', conn, if_exists='replace', index=False)\n",
        "\n",
        "# Close the connection\n",
        "conn.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Connect to the database\n",
        "conn = sqlite3.connect('database.db')\n",
        "\n",
        "# Read the data in chunks\n",
        "chunks = pd.read_csv(\"https://bit.ly/3H2XVgC\", chunksize=1000)\n",
        "\n",
        "# Iterate over the chunks\n",
        "for i, chunk in enumerate(chunks):\n",
        "    # Load the chunk into a new table in the database\n",
        "    chunk.to_sql(f'table_{i}', conn, if_exists='replace', index=False)\n",
        "\n",
        "# Close the connection\n",
        "conn.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "6KBtHvjXtzf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "# Connect to the database\n",
        "conn = sqlite3.connect('database.db')\n",
        "\n",
        "# Query the table and retrieve the column information\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"SELECT * FROM PRAGMA table_3('table_3')\")\n",
        "columns = cursor.fetchall()\n",
        "\n",
        "# Print the column information\n",
        "for column in columns:\n",
        "    print(f'Column name: {column[1]}, data type: {column[2]}')\n",
        "\n",
        "# Close the connection\n",
        "conn.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "Aw2wClBLuYo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt1jPEhseD8r"
      },
      "source": [
        "## Question 4: Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggt-hEebeJGr"
      },
      "source": [
        "Now that the data is in SQLite, we can use the pandas SQLite workflow we learned in the last lesson to explore and analyze startup investments. Remember that each row isn't a unique company, but a unique investment from a single investor. This means that many startups will span multiple rows.\n",
        "\n",
        "Use the pandas SQLite workflow to answer the following questions:\n",
        "\n",
        "* What proportion of the total amount of funds did the top 10% raise? What about the top 1%? Compare these values to the proportions the bottom 10% and bottom 1% raised.\n",
        "* Which category of company attracted the most investments?\n",
        "* Which investor contributed the most money (across all startups)?\n",
        "* Which investors contributed the most money per startup?\n",
        "* Which funding round was the most popular? Which was the least popular?\n",
        "\n",
        "Here are some ideas for further exploration:\n",
        "\n",
        "* Repeat the tasks in this project using stricter memory constraints (under 1 megabyte).\n",
        "* Clean and analyze the other Crunchbase data sets from the same GitHub repo.\n",
        "* Understand which columns the data sets share, and how the data sets are linked.\n",
        "* Create a relational database design that links the data sets together and reduces the overall disk space the database file consumes.\n",
        "\n",
        "Use pandas to populate each table in the database, create the appropriate indexes, and so on."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the investment amount at the 90th and 99th percentiles\n",
        "top_10_threshold = df['raised_amount_usd'].quantile(0.90)\n",
        "top_1_threshold = df['raised_amount_usd'].quantile(0.99)\n",
        "\n",
        "# Calculate the total investment amount\n",
        "total_investment = df['raised_amount_usd'].sum()\n",
        "\n",
        "# Calculate the proportion of funds raised by the top 10% of startups\n",
        "top_10_proportion = df[df['raised_amount_usd'] >= top_10_threshold]['raised_amount_usd'].sum() / total_investment\n",
        "print(f'The top 10% of startups raised {top_10_proportion:.2%} of the total funds.')\n",
        "\n",
        "# Calculate the proportion of funds raised by the top 1% of startups\n",
        "top_1_proportion = df[df['raised_amount_usd'] >= top_1_threshold]['raised_amount_usd'].sum() / total_investment\n",
        "print(f'The top 1% of startups raised {top_1_proportion:.2%} of the total funds.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOQJrh64S8Fl",
        "outputId": "589efc9c-18b4-4a12-cb7e-ed643e0d1a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top 10% of startups raised 51.22% of the total funds.\n",
            "The top 1% of startups raised 19.35% of the total funds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data into a pandas DataFrame\n",
        "df = pd.read_sql_query(\"SELECT * FROM table_name\", conn)\n",
        "\n",
        "# Print the column names\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "id": "0WJ9N6kyB_S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "# Connect to the database\n",
        "conn = sqlite3.connect('database.db')\n",
        "\n",
        "# Create a cursor\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Execute a SELECT statement to retrieve the names of all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "\n",
        "# Fetch all the rows from the SELECT statement\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "# Print the names of all tables\n",
        "for table in tables:\n",
        "    print(table[0])\n",
        "\n",
        "# Close the cursor and connection\n",
        "cursor.close()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "hcMaI6txAk9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by category and calculate the total investment amount for each category\n",
        "category_investments = df.groupby('company_category_code')['raised_amount_usd'].sum()\n",
        "\n",
        "\n",
        "# Find the categories with the maximum total investment amount\n",
        "most_invested_categories = category_investments[category_investments == category_investments.max()]\n",
        "\n",
        "print(f'The category that attracted the most investments was {most_invested_categories}.')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pTIsdZHM1nrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by investor and calculate the total investment amount for each investor\n",
        "investor_investments = df.groupby('investor')['amount'].sum()\n",
        "\n",
        "# Find the investor with the maximum total investment amount\n",
        "top_investor = investor_investments.idxmax()\n",
        "\n",
        "\n",
        "print(f'The investor who contributed the most money was {top_investor}.')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p8zPJGkU4cMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "conn = sqlite3.connect(\"startup_investments.db\")\n",
        "cur = conn.cursor()\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_sql_query(\"SELECT * FROM startup\", conn)\n",
        "\n",
        "investor_groups = df.groupby(\"investor_name\")\n",
        "investment_totals = investor_groups[\"investment_amount\"].sum()\n",
        "\n",
        "investment_totals = investment_totals.sort_values(ascending=False)\n",
        "\n",
        "top_investor = investment_totals.index[0]\n",
        "print(f\"The top investor is {top_investor} with a total investment of ${investment_totals[0]:.2f}.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gliSw_PkdFHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "conn = sqlite3.connect(\"startup_investments.db\")\n",
        "cur = conn.cursor()\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_sql_query(\"SELECT * FROM investments\", conn)\n",
        "\n",
        "round_groups = df.groupby(\"funding_round\")\n",
        "investment_counts = round_groups.size()\n",
        "\n",
        "investment_counts = investment_counts.sort_values(ascending=False)\n",
        "\n",
        "\n",
        "most_popular = investment_counts.index[0]\n",
        "least_popular = investment_counts.index[-1]\n",
        "print(f\"The most popular funding round is {most_popular} with {investment_counts[0]} investments.\")\n",
        "print(f\"The least popular funding round is {least_popular} with {investment_counts[-1]} investments.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qePqXm28ex0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Repeat the tasks in this project using stricter memory constraints (under 1 megabyte).\n",
        "\n",
        "import sqlite3\n",
        "conn = sqlite3.connect(\"startup_investments.db\")\n",
        "cur = conn.cursor()\n",
        "\n",
        "cur.execute(\"SELECT investor_name, funding_round, investment_amount FROM investments\")\n",
        "data = cur.fetchall()\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data, columns=[\"investor_name\", \"funding_round\", \"investment_amount\"])\n",
        "\n",
        "\n",
        "cur.close()\n",
        "conn.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IZx94K0YgRWg"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ms8zSGalg8Dm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}